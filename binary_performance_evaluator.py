"""binary_performance_evaluator.py
-----------------------------------
A self‑contained utility to evaluate the performance of an *already‑trained* binary
classifier across train / test / (optional) validation datasets.

Requirements
------------
Python >= 3.9
pandas, numpy, scikit‑learn, matplotlib, seaborn, plotly, kaleido

Quick Example
-------------
from binary_performance_evaluator import BinaryPerformanceEvaluator

evaluator = BinaryPerformanceEvaluator(
    model='modelo_treinado.pkl',
    df_train=df_train,
    df_test=df_test,
    df_val=df_val,                 # optional
    target_col='default_90d',
    id_cols=['contract_id'],
    date_col='snapshot_date',      # optional
    group_col='product_type',      # optional
    save_dir='figs',               # optional
    threshold=0.5                  # optional
)

evaluator.compute_metrics()
evaluator.plot_confusion(save=True)
evaluator.plot_calibration()
evaluator.plot_event_rate()
evaluator.plot_psi()
evaluator.plot_ks()

print(evaluator.report)           # full dict of results
"""

from __future__ import annotations

import pickle
import warnings
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Union

import joblib
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import seaborn as sns
from decile_plot import decile_analysis_plot
from optbinning import OptimalBinning
from sklearn.calibration import calibration_curve
from sklearn.metrics import (
    average_precision_score,
    brier_score_loss,
    confusion_matrix,
    matthews_corrcoef,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve,
)

sns.set(style="whitegrid")  # consistent style throughout


def _psi_single(p_base: np.ndarray, p_test: np.ndarray) -> float:
    """Compute PSI for two proportional histograms (no zeros allowed)."""
    mask = (p_base > 0) & (p_test > 0)
    if not mask.any():
        return 0.0
    delta = (p_test[mask] - p_base[mask]) * np.log(p_test[mask] / p_base[mask])
    return float(delta.sum())


def _rgba(color: str, alpha: float) -> str:
    """Return RGBA string with given opacity from ``rgb(r,g,b)`` input."""
    if color.startswith("rgb(") and color.endswith(")"):
        return color.replace("rgb", "rgba").replace(")", f",{alpha})")
    return color


class BinaryPerformanceEvaluator:
    """Evaluate binary classifier performance on multiple splits.

    Parameters
    ----------
    model : Union[str, Path, object]
        Path to `.joblib`/`.pkl` file **or** an in‑memory estimator that
        implements `predict_proba` (usual scikit‑learn API).
    df_train : pd.DataFrame
        Training set including predictors + target column.
    df_test : pd.DataFrame
        Test set including predictors + target column.
    df_val : Optional[pd.DataFrame], default=None
        Optional validation set.
    target_col : str
        Name of the binary target column (0 = negative, 1 = positive).
    id_cols : List[str]
        Columns that uniquely identify instances (e.g., contract or customer id).
    date_col : Optional[str], default=None
        Datetime column for temporal analyses.
    group_col : Optional[str], default=None
        Categorical column for group analyses. If ``homogeneous_group='auto'``,
        this column will be created with the labels generated by OptimalBinning.
        When ``homogeneous_group`` is ``None`` the column must already exist in
        all provided datasets.
    save_dir : Optional[str|Path], default=None
        If provided, figures are saved to this directory in PNG format.
    threshold : float, default 0.5
        Probability cutoff used to convert scores into class labels.
    homogeneous_group : str | int | pd.Series | np.ndarray | None, default "auto"
        Strategy to create homogeneous groups. See :meth:`plot_group_radar`.

    Notes
    -----
    * All DataFrames **must** contain `target_col`.
    * The class tries to automatically select predictor columns:
      - all columns present in *all* datasets
      - excluding id/date/target/group columns
    """

    ## ---------- constructor ----------
    def __init__(
        self,
        *,
        model: Union[str, Path, object],
        df_train: pd.DataFrame,
        df_test: pd.DataFrame,
        df_val: Optional[pd.DataFrame] = None,
        target_col: str,
        id_cols: List[str],
        date_col: Optional[str] = None,
        group_col: Optional[str] = None,
        save_dir: Optional[Union[str, Path]] = None,
        threshold: float = 0.5,
        homogeneous_group: Optional[Union[str, int, pd.Series, np.ndarray]] = "auto",
    ) -> None:
        self.model = self._load_model(model)
        self.df_train = df_train.copy()
        self.df_test = df_test.copy()
        self.df_val = df_val.copy() if df_val is not None else None
        self.target_col = target_col
        self.id_cols = id_cols
        self.date_col = date_col
        self.group_col = group_col
        self.threshold = threshold
        self.homogeneous_group = homogeneous_group

        self.save_dir = Path(save_dir) if save_dir is not None else None
        if self.save_dir:
            self.save_dir.mkdir(parents=True, exist_ok=True)

        if hasattr(self.model, "classes_") and 1 in getattr(self.model, "classes_"):
            self._pos_class_idx = list(self.model.classes_).index(1)
        else:
            self._pos_class_idx = 1

        self._validate_data()
        self._parse_date_col()
        self.predictor_cols = self._infer_predictors()
        self.model_feature_names = self._get_model_feature_names()
        self.model_n_features = self._get_model_n_features()
        self.predictor_cols = self._align_predictors_with_model(self.predictor_cols)
        self._validate_predictors()
        self.report: Dict[str, Dict[str, float]] = {}
        self.score_col_ = "y_pred_proba"
        self.label_col_ = "y_pred_label"
        self.group_col_ = self.group_col if self.group_col else "homogeneous_group"
        self.group_: Dict[str, pd.Series] | None = None
        self.binning_table_: Any | None = None
        self.group_palette_: Dict[Any, str] | None = None

        self._score_datasets()
        self._assign_groups()

    ## ---------- public API ----------
    def compute_metrics(self) -> pd.DataFrame:
        """Compute metrics and return them as a DataFrame."""
        self._validate_predictors()
        splits = {"train": self.df_train, "test": self.df_test}
        if self.df_val is not None:
            splits["val"] = self.df_val

        records = []
        for split_name, df in splits.items():
            y_true = df[self.target_col].values
            y_pred_proba = df[self.score_col_].values

            metrics_dict = {
                "MCC": matthews_corrcoef(
                    y_true, (y_pred_proba >= self.threshold).astype(int)
                ),
                "AUC_ROC": roc_auc_score(y_true, y_pred_proba),
                "AUC_PR": average_precision_score(y_true, y_pred_proba),
                "Precision": precision_score(
                    y_true, (y_pred_proba >= self.threshold).astype(int)
                ),
                "Recall": recall_score(
                    y_true, (y_pred_proba >= self.threshold).astype(int)
                ),
                "Brier": brier_score_loss(y_true, y_pred_proba),
            }
            self.report[split_name] = metrics_dict
            records.append({"Split": split_name.capitalize(), **metrics_dict})

        return pd.DataFrame(records).set_index("Split")

    def binning_table(self) -> Any | None:
        """Return the binning table used for homogeneous groups."""
        return self.binning_table_

    def plot_confusion(
        self,
        y_true: pd.Series,
        y_pred_proba: np.ndarray,
        *,
        threshold: float | str = 0.5,
        normalize: bool = False,
        title: str = "",
        cmap: str = "Blues",
        group_col: Optional[pd.Series] = None,
    ) -> go.Figure | Dict[Any, go.Figure]:
        """Return confusion matrix figure(s).

        Parameters
        ----------
        y_true : pd.Series
            True binary labels.
        y_pred_proba : np.ndarray
            Predicted probabilities for the positive class.
        threshold : float | {"ks", "youden"}, default 0.5
            Probability threshold or method to determine it.
        normalize : bool, default False
            If ``True``, heatmap values are percentages.
        title : str, default ""
            Figure title.
        cmap : str, default "Blues"
            Plotly colour scale name.
        group_col : pd.Series, optional
            When provided, one figure per group will be returned.
        """

        def _best_threshold(method: str) -> float:
            fpr, tpr, thr = roc_curve(y_true, y_pred_proba)
            if method == "youden":
                score = tpr - fpr
            else:
                score = tpr - fpr
            return float(thr[np.argmax(score)])

        if isinstance(threshold, str):
            threshold = _best_threshold(threshold.lower())

        if group_col is not None:
            figs: Dict[Any, go.Figure] = {}
            for group, mask in group_col.groupby(group_col).groups.items():
                figs[group] = self.plot_confusion(
                    y_true[mask],
                    y_pred_proba[mask],
                    threshold=threshold,
                    normalize=normalize,
                    title=f"{title} - {group}" if title else str(group),
                    cmap=cmap,
                )
            return figs

        y_pred = (y_pred_proba >= float(threshold)).astype(int)
        cm_abs = confusion_matrix(y_true, y_pred, labels=[0, 1])
        cm_pct = cm_abs / cm_abs.sum()
        matrix = cm_pct if normalize else cm_abs

        fig = go.Figure(
            data=go.Heatmap(
                z=matrix,
                x=["Pred 0", "Pred 1"],
                y=["True 0", "True 1"],
                colorscale=cmap,
                showscale=False,
            )
        )
        fig.update_yaxes(autorange="reversed")

        for i in range(2):
            for j in range(2):
                fig.add_annotation(
                    x=j,
                    y=i,
                    text=f"{cm_abs[i, j]:,}\n({100 * cm_pct[i, j]:.1f}%)",
                    showarrow=False,
                    font=dict(color="white" if cm_pct[i, j] > 0.5 else "black"),
                )

        fig.update_layout(
            title=title or "Confusion Matrix",
            xaxis_title="Predicted label",
            yaxis_title="True label",
            template="plotly_white",
        )
        return fig

    def plot_calibration(
        self, *, n_bins: int = 10, save: bool = False, title: str = ""
    ) -> go.Figure:
        """Reliability diagram for test split using Plotly."""
        self._validate_predictors()
        y_true = self.df_test[self.target_col].values
        y_pred_proba = self.model.predict_proba(
            self.df_test[self.predictor_cols]
        )[:, self._pos_class_idx]
        prob_true, prob_pred = calibration_curve(
            y_true,
            y_pred_proba,
            n_bins=n_bins,
            strategy="uniform",
        )

        brier = brier_score_loss(y_true, y_pred_proba)

        fig = go.Figure()
        fig.add_trace(
            go.Scatter(x=prob_pred, y=prob_true, mode="lines+markers", name="Model")
        )
        fig.add_trace(
            go.Scatter(
                x=[0, 1], y=[0, 1], mode="lines", line=dict(dash="dash"), name="Ideal"
            )
        )
        fig.update_layout(
            title=title or f"Calibration Curve – Test (Brier = {brier:.4f})",
            xaxis_title="Predicted probability",
            yaxis_title="Observed frequency",
            template="plotly_white",
        )

        if save and self.save_dir:
            fig.write_image(str(self.save_dir / "calibration_curve.png"))
        return fig

    def plot_event_rate(
        self, *, save: bool = False, title: str = ""
    ) -> tuple[go.Figure, go.Figure]:
        """Return two figures with event rate and group share over time."""
        if self.date_col is None:
            raise ValueError("`date_col` is required for plot_event_rate().")

        group_col = None
        for cand in [self.group_col, self.group_col_]:
            if cand and cand in self.data_.columns:
                group_col = cand
                break
        if group_col is None:
            raise ValueError("Group column not found for plot_event_rate().")

        df_all = pd.concat(
            [
                self.df_train.assign(Split="Train"),
                self.df_test.assign(Split="Test"),
                *([self.df_val.assign(Split="Val")] if self.df_val is not None else []),
            ],
            axis=0,
        )
        df_all[self.date_col] = pd.to_datetime(df_all[self.date_col])

        groups = sorted(df_all[group_col].unique())

        pivot = (
            df_all.groupby([self.date_col, group_col])[self.target_col]
            .mean()
            .unstack(group_col)
            .reindex(columns=groups)
            .sort_index()
        )

        counts = (
            df_all.groupby([self.date_col, group_col])
            .size()
            .unstack(group_col)
            .reindex(columns=groups, fill_value=0)
            .sort_index()
        )
        pct = counts.div(counts.sum(axis=1), axis=0)

        self._compute_group_palette()
        colors = self.group_palette_ or {}

        fig_rate = go.Figure()
        for col in pivot.columns:
            fig_rate.add_trace(
                go.Scatter(
                    x=pivot.index,
                    y=pivot[col],
                    mode="lines+markers",
                    name=str(col),
                    line=dict(color=colors.get(col)),
                )
            )
        fig_rate.update_layout(
            title=title or "Event Rate by Group over Time",
            yaxis_title="Event rate",
            xaxis_title=self.date_col,
            template="plotly_white",
        )

        fig_share = go.Figure()
        for col in pct.columns:
            fig_share.add_trace(
                go.Bar(
                    x=pct.index,
                    y=pct[col],
                    name=str(col),
                    marker=dict(color=colors.get(col)),
                )
            )
        fig_share.update_layout(
            barmode="stack",
            title="Group Share over Time",
            yaxis_title="Group share",
            yaxis_tickformat=".0%",
            xaxis_title=self.date_col,
            template="plotly_white",
        )

        if save and self.save_dir:
            fig_rate.write_image(str(self.save_dir / "event_rate.png"))
            fig_share.write_image(str(self.save_dir / "group_share.png"))

        fig_rate.show()
        fig_share.show()

        return fig_rate, fig_share

    def plot_psi(
        self,
        *,
        reference_df: Optional[pd.DataFrame] = None,
        bin_strategy: Optional[Dict[str, Any]] = None,
        min_obs: int = 100,
        eps: float = 1e-9,
        save: bool = False,
        title: str = "",
    ) -> go.Figure:
        """Compute and plot PSI per variable through time using Plotly.

        Parameters
        ----------
        reference_df : pd.DataFrame, optional
            Dataset used as reference for binning. Defaults to ``df_train``.
        bin_strategy : dict, optional
            ``{"type": "quantile", "n_bins": 10}`` or ``{"type": "fixed"}``.
        min_obs : int, default 100
            Minimum observations required per period to compute PSI.
        eps : float, default 1e-9
            Small constant added to counts to avoid zeros.
        save : bool, default False
            If ``True`` and ``save_dir`` is set, image is written to disk.
        """

        if self.date_col is None:
            raise ValueError("`date_col` is required for plot_psi().")

        reference_df = reference_df if reference_df is not None else self.df_train
        bin_strategy = bin_strategy or {"type": "quantile", "n_bins": 10}

        periods = (
            pd.to_datetime(self.df_test[self.date_col])
            .dt.to_period("M")
            .sort_values()
            .unique()
        )

        psi_records: List[Dict[str, Any]] = []
        for var in self._psi_variables():
            ref_series = pd.to_numeric(reference_df[var], errors="coerce").dropna()
            if ref_series.empty:
                continue

            if bin_strategy.get("type") == "quantile":
                try:
                    _, edges = pd.qcut(
                        ref_series,
                        q=bin_strategy.get("n_bins", 10),
                        retbins=True,
                        duplicates="drop",
                    )
                except ValueError:
                    edges = np.linspace(
                        ref_series.min(),
                        ref_series.max(),
                        bin_strategy.get("n_bins", 10) + 1,
                    )
            else:
                edges = np.linspace(
                    ref_series.min(),
                    ref_series.max(),
                    bin_strategy.get("n_bins", 10) + 1,
                )

            edges[0] = min(edges[0], ref_series.min())
            edges[-1] = max(edges[-1], ref_series.max())

            counts = np.histogram(ref_series, bins=edges)[0].astype(float) + eps
            p_ref = counts / counts.sum()

            for period in periods:
                subset = self.df_test[
                    pd.to_datetime(self.df_test[self.date_col]).dt.to_period("M")
                    == period
                ]
                if len(subset) < min_obs:
                    continue

                ser = pd.to_numeric(subset[var], errors="coerce").dropna()
                if ser.empty:
                    continue

                edges_adj = edges.copy()
                if ser.min() < edges_adj[0]:
                    edges_adj[0] = ser.min()
                if ser.max() > edges_adj[-1]:
                    edges_adj[-1] = ser.max()

                counts_test = np.histogram(ser, bins=edges_adj)[0].astype(float) + eps
                p_test = counts_test / counts_test.sum()

                psi = _psi_single(p_ref, p_test)
                psi_records.append(
                    {
                        "Variable": var,
                        "Period": period.to_timestamp(),
                        "PSI": psi,
                    }
                )

        psi_df = pd.DataFrame(psi_records)
        if psi_df.empty:
            warnings.warn("PSI could not be computed (insufficient data).")
            return go.Figure()

        fig = go.Figure()
        for var, grp in psi_df.groupby("Variable"):
            fig.add_trace(
                go.Scatter(
                    x=grp["Period"],
                    y=grp["PSI"],
                    mode="lines+markers",
                    name=str(var),
                )
            )

        fig.add_hline(
            y=0.1,
            line=dict(color="orange", dash="dash"),
            annotation_text="0.10",
        )
        fig.add_hline(
            y=0.25,
            line=dict(color="red", dash="dash"),
            annotation_text="0.25",
        )
        fig.update_layout(
            title=title or "PSI over Time by Variable",
            xaxis_title="Period",
            yaxis_title="Population Stability Index",
            template="plotly_white",
        )
        if save and self.save_dir:
            fig.write_image(str(self.save_dir / "psi_over_time.png"))
        return fig

    def plot_ks(self, *, save: bool = False, title: str = "") -> go.Figure:
        """KS statistic over time for each split using Plotly."""
        if self.date_col is None:
            raise ValueError("`date_col` is required for plot_ks().")
        self._validate_predictors()

        def ks_stat(y_true: np.ndarray, y_pred: np.ndarray) -> float:
            fpr, tpr, _ = roc_curve(y_true, y_pred)
            return np.max(np.abs(tpr - fpr))

        dfs = [
            ("Train", self.df_train),
            ("Test", self.df_test),
            *([("Val", self.df_val)] if self.df_val is not None else []),
        ]

        ks_records = []
        for split_name, df in dfs:
            df = df.copy()
            df["Period"] = pd.to_datetime(df[self.date_col]).dt.to_period("M")
            for period, grp in df.groupby("Period"):
                y_true = grp[self.target_col].values
                y_pred = self.model.predict_proba(grp[self.predictor_cols])[:, self._pos_class_idx]
                ks = ks_stat(y_true, y_pred)
                ks_records.append(
                    {"Split": split_name, "Period": period.to_timestamp(), "KS": ks}
                )

        ks_df = pd.DataFrame(ks_records)
        if ks_df.empty:
            warnings.warn("KS could not be computed (insufficient data).")
            return

        fig = go.Figure()
        for split, grp in ks_df.groupby("Split"):
            fig.add_trace(
                go.Scatter(
                    x=grp["Period"],
                    y=grp["KS"],
                    mode="lines+markers",
                    name=str(split),
                )
            )
        fig.add_hline(
            y=0.05, line=dict(color="gray", dash="dash"), annotation_text="0.05"
        )
        fig.update_layout(
            title=title or "KS Evolution over Time",
            xaxis_title="Period",
            yaxis_title="Kolmogorov–Smirnov",
            template="plotly_white",
        )
        if save and self.save_dir:
            fig.write_image(str(self.save_dir / "ks_evolution.png"))
        return fig

    def plot_group_radar(
        self,
        features: List[str] | None = None,
        *,
        scaler: Literal["zscore", "minmax"] = "zscore",
        save: bool = False,
        title: str = "",
    ) -> go.Figure:
        """Return radar chart of average feature values per homogeneous group."""
        if self.group_ is None:
            raise ValueError("Homogeneous groups were not computed.")

        self._compute_group_palette()

        df = self.data_.copy()
        if features is None:
            numeric_predictors = [
                c for c in self.predictor_cols if pd.api.types.is_numeric_dtype(df[c])
            ]
            features = numeric_predictors
        if not features:
            raise ValueError("No numeric features available for radar plot.")

        if scaler == "zscore":
            scaled = df[features].apply(lambda x: (x - x.mean()) / x.std(ddof=0))
        else:
            scaled = df[features].apply(lambda x: (x - x.min()) / (x.max() - x.min()))

        mean_by_group = scaled.groupby(df[self.group_col_])[features].mean()

        fig = go.Figure()
        for group_id, row in mean_by_group.iterrows():
            fig.add_trace(
                go.Scatterpolar(
                    r=row.values,
                    theta=features,
                    fill="toself",
                    name=f"Group {group_id}",
                    line=dict(
                        color=_rgba(self.group_palette_.get(group_id), 0.8)
                    ),
                    fillcolor=_rgba(self.group_palette_.get(group_id), 0.3),
                )
            )

        fig.update_layout(
            template="plotly_white",
            title=title or "Group Radar",
            polar=dict(radialaxis=dict(showticklabels=False)),
        )
        if save and self.save_dir:
            fig.write_image(str(self.save_dir / "group_radar.png"))
        return fig

    def plot_decile_ks(
        self,
        *,
        n_bins: int = 10,
        ascending: bool = True,
        group_id: int | None = None,
        title: str = "",
        **kwargs: Any,
    ) -> go.Figure:
        """Wrapper around :func:`decile_analysis_plot` respecting groups."""
        df = self.data_.copy()
        if group_id is not None:
            if self.group_ is None:
                raise ValueError("Homogeneous groups were not computed.")
            df = df[df[self.group_col_] == group_id]

        fig, _, _ = decile_analysis_plot(
            df,
            score_col=self.score_col_,
            target_col=self.target_col,
            n_bins=n_bins,
            ascending=ascending,
            title_prefix=title or "Ordenação por decil",
            **kwargs,
        )
        return fig

    ## ---------- helpers ----------
    def _load_model(self, model: Union[str, Path, object]):
        """Load model from path or return object as‑is."""
        if isinstance(model, (str, Path)):
            model_path = Path(model)
            if model_path.suffix in {".joblib", ".jbl"}:
                return joblib.load(model_path)
            elif model_path.suffix in {".pkl", ".pickle"}:
                with open(model_path, "rb") as f:
                    return pickle.load(f)
            else:
                raise ValueError(
                    f"Unsupported model file extension: {model_path.suffix}"
                )
        else:
            # assume object is already a fitted estimator
            if not hasattr(model, "predict_proba"):
                raise AttributeError("Provided model object lacks predict_proba.")
            return model

    def _validate_data(self) -> None:
        """Basic dataframe validations."""
        for name, df in [
            ("df_train", self.df_train),
            ("df_test", self.df_test),
            ("df_val", self.df_val),
        ]:
            if df is None:
                continue
            if self.target_col not in df.columns:
                raise KeyError(f"{self.target_col} missing in {name}.")
            if df[self.target_col].isna().any():
                raise ValueError(f"NaN detected in target column of {name}.")

            missing_ids = [col for col in self.id_cols if col not in df.columns]
            if missing_ids:
                raise KeyError(f"{name} missing id_cols: {missing_ids}")

        if self.homogeneous_group is None:
            if not self.group_col:
                raise ValueError(
                    "`group_col` must be provided when homogeneous_group is None."
                )
            for name, df in [
                ("df_train", self.df_train),
                ("df_test", self.df_test),
                ("df_val", self.df_val),
            ]:
                if df is None:
                    continue
                if self.group_col not in df.columns:
                    raise KeyError(f"{name} missing group_col: {self.group_col}")

    def _parse_date_col(self) -> None:
        """Parse `date_col` to datetime when format yyyymm is detected."""
        if not self.date_col:
            return

        for df in [self.df_train, self.df_test, self.df_val]:
            if df is None or self.date_col not in df.columns:
                continue

            col = df[self.date_col]
            if pd.api.types.is_integer_dtype(col) or pd.api.types.is_float_dtype(col):
                try:
                    df[self.date_col] = pd.to_datetime(
                        col.astype(int).astype(str), format="%Y%m"
                    )
                    continue
                except Exception:
                    pass
            df[self.date_col] = pd.to_datetime(col, errors="coerce")

    def _infer_predictors(self) -> List[str]:
        """Infer intersection of columns across all datasets, excluding id/date/group/target."""
        cols = set(self.df_train.columns)
        cols &= set(self.df_test.columns)
        if self.df_val is not None:
            cols &= set(self.df_val.columns)

        exclude = set(self.id_cols + [self.target_col])
        if self.date_col:
            exclude.add(self.date_col)
        if self.group_col:
            exclude.add(self.group_col)

        predictor_cols = sorted(list(cols - exclude))
        if not predictor_cols:
            raise ValueError("No predictor columns detected after exclusions.")
        return predictor_cols

    def _get_model_feature_names(self) -> Optional[List[str]]:
        """Return feature names used during model training, if available."""
        if hasattr(self.model, "feature_names_in_"):
            return list(getattr(self.model, "feature_names_in_"))
        try:
            booster = self.model.get_booster()
            if hasattr(booster, "feature_names"):
                return list(booster.feature_names)
        except Exception:
            pass
        return None

    def _get_model_n_features(self) -> Optional[int]:
        """Return the number of features the model expects, if available."""
        if hasattr(self.model, "n_features_in_"):
            return int(getattr(self.model, "n_features_in_"))
        try:
            booster = self.model.get_booster()
            if hasattr(booster, "num_features"):
                return int(booster.num_features())
        except Exception:
            pass
        return None

    def _align_predictors_with_model(self, cols: List[str]) -> List[str]:
        """Ensure predictors match model's training features, preserving order."""
        if self.model_feature_names:
            missing = [c for c in self.model_feature_names if c not in cols]
            if missing:
                raise ValueError(
                    f"Model expects columns not present in provided data: {missing}"
                )
            return [c for c in self.model_feature_names if c in cols]

        if self.model_n_features is not None and len(cols) != self.model_n_features:
            raise ValueError(
                f"Number of predictor columns ({len(cols)}) does not match model "
                f"expectation ({self.model_n_features})."
            )
        return cols

    def _validate_predictors(self) -> None:
        """Ensure predictor columns align with model expectations."""
        for name, df in [
            ("df_train", self.df_train),
            ("df_test", self.df_test),
            ("df_val", self.df_val),
        ]:
            if df is None:
                continue
            missing = [c for c in self.predictor_cols if c not in df.columns]
            if missing:
                raise KeyError(f"{name} missing predictor columns: {missing}")

        if self.model_feature_names:
            ordered = [c for c in self.model_feature_names if c in self.predictor_cols]
            if ordered != self.predictor_cols:
                self.predictor_cols = ordered
        elif (
            self.model_n_features is not None
            and len(self.predictor_cols) != self.model_n_features
        ):
            raise ValueError(
                f"Model expects {self.model_n_features} features, got {len(self.predictor_cols)}"
            )

    def _score_datasets(self) -> None:
        """Add predicted probabilities and labels to each split."""
        dfs = [("train", self.df_train), ("test", self.df_test)]
        if self.df_val is not None:
            dfs.append(("val", self.df_val))

        for name, df in dfs:
            proba = self.model.predict_proba(df[self.predictor_cols])[:, self._pos_class_idx]
            df[self.score_col_] = proba
            df[self.label_col_] = (proba >= self.threshold).astype(int)
            df["Split"] = name.capitalize()

        self.data_ = pd.concat(
            [self.df_train, self.df_test]
            + ([self.df_val] if self.df_val is not None else []),
            axis=0,
            ignore_index=True,
        )

    def _assign_groups(self) -> None:
        """Create homogeneous groups according to ``self.homogeneous_group``."""
        if self.homogeneous_group is None:
            return

        self.group_ = {}

        if isinstance(self.homogeneous_group, str):
            if self.homogeneous_group != "auto":
                raise ValueError("Unsupported string for homogeneous_group")

            optb = OptimalBinning(
                name="y_proba_train",
                dtype="numerical",
                solver="mip",
                min_prebin_size=0.01,
                max_n_bins=5,
                min_bin_size=0.05,
                monotonic_trend="descending",
            )
            optb.fit(
                self.df_train[self.score_col_] * 1000, self.df_train[self.target_col]
            )
            self.binning_table_ = optb.binning_table.build()

            for name, df in [
                ("train", self.df_train),
                ("test", self.df_test),
                ("val", self.df_val),
            ]:
                if df is None:
                    continue
                labels = optb.transform(df[self.score_col_] * 1000, metric="bins")
                df[self.group_col_] = labels
                self.group_[name] = labels

        elif isinstance(self.homogeneous_group, int):
            n = int(self.homogeneous_group)
            for name, df in [
                ("train", self.df_train),
                ("test", self.df_test),
                ("val", self.df_val),
            ]:
                if df is None:
                    continue
                bins = pd.qcut(
                    df[self.score_col_].rank(method="first"),
                    q=n,
                    labels=range(1, n + 1),
                ).astype(int)
                df[self.group_col_] = bins
                self.group_[name] = bins
            self.binning_table_ = None

        else:
            groups = pd.Series(self.homogeneous_group)
            if len(groups) != len(self.data_):
                raise ValueError("Length of provided group labels does not match data.")

            self.data_[self.group_col_] = groups.reset_index(drop=True)
            start = 0
            for name, df in [
                ("train", self.df_train),
                ("test", self.df_test),
                ("val", self.df_val),
            ]:
                if df is None:
                    continue
                end = start + len(df)
                df[self.group_col_] = groups.iloc[start:end].values
                self.group_[name] = df[self.group_col_]
                start = end
            self.binning_table_ = None

        self.data_ = pd.concat(
            [self.df_train, self.df_test]
            + ([self.df_val] if self.df_val is not None else []),
            axis=0,
            ignore_index=True,
        )
        self._compute_group_palette()

    def _compute_group_palette(self) -> None:
        """Create consistent colours per group based on event rate."""
        group_col = None
        for cand in [self.group_col, self.group_col_]:
            if cand and cand in self.data_.columns:
                group_col = cand
                break
        if group_col is None:
            return

        rates = self.data_.groupby(group_col)[self.target_col].mean().sort_values()
        palette = sns.diverging_palette(240, 10, n=len(rates))
        self.group_palette_ = {
            grp: f"rgb({int(r*255)},{int(g*255)},{int(b*255)})"
            for grp, (r, g, b) in zip(rates.index, palette)
        }

    def _psi_variables(self) -> List[str]:
        """Select variables to evaluate for PSI (exclude id/date/target)."""
        exclude = set(self.id_cols + [self.target_col])
        if self.date_col:
            exclude.add(self.date_col)
        vars_ = [c for c in self.df_train.columns if c not in exclude]
        numeric_vars = [
            v for v in vars_ if pd.api.types.is_numeric_dtype(self.df_train[v])
        ]
        return numeric_vars
